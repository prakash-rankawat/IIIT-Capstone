{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "12daca49",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading necessary libraries\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "from textblob import TextBlob\n",
    "from textblob.sentiments import NaiveBayesAnalyzer\n",
    "import nltk\n",
    "from spellchecker import SpellChecker\n",
    "from collections import Counter\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import numpy as np\n",
    "import readability\n",
    "import csv\n",
    "import requests as r\n",
    "from datetime import datetime\n",
    "import time\n",
    "import warnings\n",
    "from urllib.parse import urljoin\n",
    "from urllib.parse import urlparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cc7c4348",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_extract(url,save_flag=True):\n",
    "        \n",
    "    # Get the text of web page\n",
    "    # instantiate a BeautifulSoup object\n",
    "    source = requests.get(url).text\n",
    "    soup = BeautifulSoup(source,\"html.parser\")\n",
    "    \n",
    "    #to find the Stylesheets\n",
    "    #links = soup.find_all(\"link\", {\"rel\":\"stylesheet\"})\n",
    "    #numCSS = len(links)\n",
    "   \n",
    "    # strip all script and style elements\n",
    "    for script in soup([\"script\", \"style\"]):\n",
    "        script.decompose()\n",
    "\n",
    "    arr = soup.get_text(\" \", strip=True).split('\\n')\n",
    "    \n",
    "    # discard empty rows\n",
    "    arr = list(filter(lambda x: len(x) > 0, arr))\n",
    "    \n",
    "    # remove leading traing white spaces\n",
    "    arr = list(map(lambda x: x.strip(), arr))\n",
    "    \n",
    "    # Extract text\n",
    "    text = \"\"\n",
    "    for sentence in arr:\n",
    "        text += sentence + \" \"\n",
    "    \n",
    "    # Text based features\n",
    "    # token count\n",
    "    token_count = len(text)\n",
    "    \n",
    "    # commas count\n",
    "    commas_count = len(re.findall(',', text))\n",
    "    \n",
    "    # exclamations count\n",
    "    exclamations_count = len(re.findall('!', text))\n",
    "    \n",
    "    # dots count\n",
    "    dots_count = len(re.findall('\\.', text))\n",
    "    \n",
    "    # questions count\n",
    "    questions_count = len(re.findall(\"\\?\", text))\n",
    "    \n",
    "    # polarity\n",
    "    text_blob = TextBlob(text)\n",
    "    polarity = text_blob.sentiment.polarity\n",
    "   \n",
    "    # split long sentences into short sentences based on '.'\n",
    "    arr = list(map(lambda x: x.split('.'), arr))\n",
    "    \n",
    "    # convert 2D list into 1D list\n",
    "    sentences_list = list()\n",
    "    for sentences_array in arr:\n",
    "        sentences_list += list(filter(lambda x: len(x) > 0, sentences_array))\n",
    "\n",
    "    \n",
    "    positive_sentences_count = 0\n",
    "    negative_sentences_count = 0\n",
    "    subjective_sentences_count = 0\n",
    "    objective_sentences_count = 0\n",
    "\n",
    "    \n",
    "    for sentence in sentences_list:\n",
    "        sent = TextBlob(sentence)\n",
    "        polarity = sent.sentiment.polarity\n",
    "        subjectivity = sent.sentiment.subjectivity\n",
    "    \n",
    "        if polarity > 0.0:\n",
    "            positive_sentences_count += 1\n",
    "        else:\n",
    "            negative_sentences_count += 1\n",
    "    \n",
    "        if subjectivity >= 0.3:\n",
    "            subjective_sentences_count += 1\n",
    "        else:\n",
    "            objective_sentences_count += 1\n",
    "        \n",
    "    text_new = \"\"\n",
    "    for word in text.split(\" \"):\n",
    "        word = re.sub(r'[^a-zA-Z]', '', word)\n",
    "        text_new += word + \" \"\n",
    " \n",
    "    # spelling errors count\n",
    "    spell = SpellChecker()\n",
    "    spelling_errors_count = 0\n",
    "    for word in text_new.split(\" \"):\n",
    "        correct_word = spell.correction(word)\n",
    "        if not word == correct_word:\n",
    "            spelling_errors_count += 1\n",
    "            \n",
    "    #Entropy(text_complexity)\n",
    "    tokenizer = TweetTokenizer()\n",
    "    tokens = tokenizer.tokenize((text))\n",
    "    num_tokens = len(tokens)\n",
    "    word_hist = Counter([token for token in tokens])\n",
    "\n",
    "    entropy_sum = 0\n",
    "    for word, count in word_hist.items():\n",
    "        entropy_sum += (count * (np.math.log10(num_tokens) - np.math.log10(count)))\n",
    "    text_complexity = (1 / num_tokens) * entropy_sum\n",
    "    \n",
    "    results = readability.getmeasures(text, lang='en')\n",
    "    results['readability grades']\n",
    "    smog = results['readability grades']['SMOGIndex']\n",
    "    \n",
    "    # POS Tagging\n",
    "    # 'Noun' : 'NN',\n",
    "    # 'Verb' : 'VB',\n",
    "    # 'Adjective' : 'JJ',\n",
    "    # 'Adverb' : 'RB',\n",
    "    # 'Determiner' : 'DT'\n",
    "    \n",
    "    #JJ\tadjective\t'big'\n",
    "    #JJR\tadjective, comparative\t'bigger'\n",
    "    #JJS\tadjective, superlative\t'biggest'\n",
    "    # NN\tnoun, singular 'desk'\n",
    "    # NNS\tnoun plural\t'desks'\n",
    "    # NNP\tproper noun, singular\t'Harrison'\n",
    "    # NNPS\tproper noun, plural\t'Americans'\n",
    "    # VB\tverb, base form\ttake\n",
    "    # VBD\tverb, past tense\ttook\n",
    "    # VBG\tverb, gerund/present participle\ttaking\n",
    "    # VBN\tverb, past participle\ttaken\n",
    "    # VBP\tverb, sing. present, non-3d\ttake\n",
    "    # VBZ\tverb, 3rd person sing. present\ttakes\n",
    "    # RB\tadverb\tvery, silently,\n",
    "    # RBR\tadverb, comparative\tbetter\n",
    "    # RBS\tadverb, superlative\tbest\n",
    "    \n",
    "    \n",
    "    # count number of nouns, verbs, adjectives, adverbs, determiners\n",
    "    for sentence in sentences_list:\n",
    "        text = nltk.word_tokenize(sentence)\n",
    "        list_of_tags = nltk.pos_tag(text)\n",
    "        noun_count=0\n",
    "        verb_count=0\n",
    "        adj_count=0\n",
    "        adv_count=0\n",
    "        deter_count=0\n",
    "    \n",
    "    for tag_tuple in list_of_tags:\n",
    "        tag = tag_tuple[1]\n",
    "        if(tag in ['NN', 'NNS', 'NNP', 'NNPS']):\n",
    "            noun_count +=1\n",
    "        elif(tag in ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']):\n",
    "            verb_count +=1\n",
    "        elif(tag in ['JJ', 'JJR', 'JJS']):\n",
    "            adj_count +=1\n",
    "        elif(tag in ['RB', 'RBR', 'RBS']):\n",
    "            adv_count +=1\n",
    "        elif(tag == 'DT'):\n",
    "            deter_count +=1\n",
    "    \n",
    "    # To get Alexa-Rank of URL\n",
    "    alex_url = \"https://www.alexa.com/siteinfo/\" + url\n",
    "    alex_respone = r.get(alex_url) # get information from page\n",
    "    alex_soup = BeautifulSoup(alex_respone.content,'html.parser')  \n",
    "    for match in alex_soup.find_all('span'): #remove all span tag\n",
    "        match.unwrap()\n",
    "    global_rank = alex_soup.select('p.big.data') # select any p tag with big and data class\n",
    "    global_rank = str(global_rank[0])\n",
    "    res = re.findall(r\"([0-9,]{1,12})\", global_rank) # find rank   \n",
    "    alexa_rank=res[0]\n",
    "\n",
    "    # To get domain name of a URL\n",
    "    get_domain = urlparse(url).netloc\n",
    "    domain = '.'.join(get_domain.split('.')[-1:])\n",
    "\n",
    "    if(save_flag == True):\n",
    "        # open the file in the write mode\n",
    "        with open('url_features.csv', 'a',newline='') as csvfile: \n",
    "            # csv header\n",
    "#            header = ['token_count','commas_count','exclamations_count','dots_count','questions_count',\n",
    "#                       'polarity','positive_sentences_count','negative_sentences_count',\n",
    "#                       'subjective_sentences_count','objective_sentences_count',\n",
    "#                       'spelling_errors_count','text_complexity','smog',\n",
    "#                       'noun_count','verb_count','adj_count','deter_count']\n",
    "            data = [url,token_count,commas_count,exclamations_count,dots_count,questions_count,\n",
    "                      polarity,positive_sentences_count,negative_sentences_count,\n",
    "                      subjective_sentences_count,objective_sentences_count,\n",
    "                      spelling_errors_count,text_complexity,smog,\n",
    "                      noun_count,verb_count,adj_count,deter_count,alexa_rank,domain]\n",
    "           \n",
    "            # create the csv writer\n",
    "            writer = csv.writer(csvfile)\n",
    "            # write header & a row to the csv file\n",
    "            # writer.writerow(header)\n",
    "            writer.writerow(data)\n",
    "    \n",
    "    feature_list=[token_count,commas_count,exclamations_count,dots_count,questions_count,\n",
    "                      polarity,positive_sentences_count,negative_sentences_count,\n",
    "                      subjective_sentences_count,objective_sentences_count,\n",
    "                      spelling_errors_count,text_complexity,smog,\n",
    "                      noun_count,verb_count,adj_count,deter_count,alexa_rank,domain]\n",
    "    \n",
    "    return feature_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b724ca28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a url : https://www.google.com\n",
      "[361, 2, 0, 2, 0, 0.0, 1, 2, 1, 2, 18, 1.7207519520660168, 8.477225575051662, 2, 0, 1, 0, '1', 'com']\n",
      "Feature list for the given URL is added to the url_features.csv file\n"
     ]
    }
   ],
   "source": [
    "# main() function\n",
    "if __name__==\"__main__\":\n",
    "    url = input('Enter a url : ')\n",
    "    feature_list = feature_extract(url,True)\n",
    "    print(feature_list)\n",
    "    print(\"Feature list for the given URL is added to the url_features.csv file\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ce9aa625",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/harshrahamatkar/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "affd04b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/harshrahamatkar/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b67fb49",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
