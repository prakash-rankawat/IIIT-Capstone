{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12daca49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspellchecker in c:\\users\\harshr-mbp_windows\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.6.3)\n",
      "Requirement already satisfied: readability in c:\\users\\harshr-mbp_windows\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.3.1)\n"
     ]
    }
   ],
   "source": [
    "#loading necessary libraries\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "from textblob import TextBlob\n",
    "from textblob.sentiments import NaiveBayesAnalyzer\n",
    "import nltk\n",
    "!pip install pyspellchecker\n",
    "from spellchecker import SpellChecker\n",
    "from collections import Counter\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import numpy as np\n",
    "!pip install readability\n",
    "import readability\n",
    "import csv\n",
    "import requests as r\n",
    "from datetime import datetime\n",
    "import time\n",
    "import warnings\n",
    "from urllib.parse import urljoin\n",
    "from urllib.parse import urlparse\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc7c4348",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_extract(url,save_flag=True):\n",
    "        \n",
    "    # Get the text of web page\n",
    "    # instantiate a BeautifulSoup object\n",
    "    source = requests.get(url).text\n",
    "    soup = BeautifulSoup(source,\"html.parser\")\n",
    "    \n",
    "    #to find the Stylesheets\n",
    "    #links = soup.find_all(\"link\", {\"rel\":\"stylesheet\"})\n",
    "    #numCSS = len(links)\n",
    "   \n",
    "    # strip all script and style elements\n",
    "    for script in soup([\"script\", \"style\"]):\n",
    "        script.decompose()\n",
    "\n",
    "    arr = soup.get_text(\" \", strip=True).split('\\n')\n",
    "    \n",
    "    # discard empty rows\n",
    "    arr = list(filter(lambda x: len(x) > 0, arr))\n",
    "    \n",
    "    # remove leading traing white spaces\n",
    "    arr = list(map(lambda x: x.strip(), arr))\n",
    "    \n",
    "    # Extract text\n",
    "    text = \"\"\n",
    "    for sentence in arr:\n",
    "        text += sentence + \" \"\n",
    "    \n",
    "    # Text based features\n",
    "    # token count\n",
    "    tokens = len(text)\n",
    "    \n",
    "    # commas count\n",
    "    commas = len(re.findall(',', text))\n",
    "    \n",
    "    # exclamations count\n",
    "    exclamations = len(re.findall('!', text))\n",
    "    \n",
    "    # dots count\n",
    "    dots = len(re.findall('\\.', text))\n",
    "    \n",
    "    # questions count\n",
    "    questions = len(re.findall(\"\\?\", text))\n",
    "    \n",
    "    # polarity\n",
    "    text_blob = TextBlob(text)\n",
    "    polarity = text_blob.sentiment.polarity\n",
    "   \n",
    "    # split long sentences into short sentences based on '.'\n",
    "    arr = list(map(lambda x: x.split('.'), arr))\n",
    "    \n",
    "    # convert 2D list into 1D list\n",
    "    sentences_list = list()\n",
    "    for sentences_array in arr:\n",
    "        sentences_list += list(filter(lambda x: len(x) > 0, sentences_array))\n",
    "\n",
    "    \n",
    "    positive_sentences = 0\n",
    "    negative_sentences = 0\n",
    "    subjective_sentences = 0\n",
    "    objective_sentences = 0\n",
    "\n",
    "    \n",
    "    for sentence in sentences_list:\n",
    "        sent = TextBlob(sentence)\n",
    "        polarity = sent.sentiment.polarity\n",
    "        subjectivity = sent.sentiment.subjectivity\n",
    "    \n",
    "        if polarity > 0.0:\n",
    "            positive_sentences += 1\n",
    "        else:\n",
    "            negative_sentences += 1\n",
    "    \n",
    "        if subjectivity >= 0.3:\n",
    "            subjective_sentences += 1\n",
    "        else:\n",
    "            objective_sentences += 1\n",
    "        \n",
    "    text_new = \"\"\n",
    "    for word in text.split(\" \"):\n",
    "        word = re.sub(r'[^a-zA-Z]', '', word)\n",
    "        text_new += word + \" \"\n",
    " \n",
    "    # spelling errors count\n",
    "    spell = SpellChecker()\n",
    "    spelling_errors = 0\n",
    "    for word in text_new.split(\" \"):\n",
    "        correct_word = spell.correction(word)\n",
    "        if not word == correct_word:\n",
    "            spelling_errors += 1\n",
    "            \n",
    "    #Entropy(text_complexity)\n",
    "    tokenizer = TweetTokenizer()\n",
    "    tokens = tokenizer.tokenize((text))\n",
    "    num_tokens = len(tokens)\n",
    "    word_hist = Counter([token for token in tokens])\n",
    "\n",
    "    entropy_sum = 0\n",
    "    for word, count in word_hist.items():\n",
    "        entropy_sum += (count * (np.math.log10(num_tokens) - np.math.log10(count)))\n",
    "    text_complexity = (1 / num_tokens) * entropy_sum\n",
    "    \n",
    "    results = readability.getmeasures(text, lang='en')\n",
    "    results['readability grades']\n",
    "    smog = results['readability grades']['SMOGIndex']\n",
    "    \n",
    "    # POS Tagging\n",
    "    # 'Noun' : 'NN',\n",
    "    # 'Verb' : 'VB',\n",
    "    # 'Adjective' : 'JJ',\n",
    "    # 'Adverb' : 'RB',\n",
    "    # 'Determiner' : 'DT'\n",
    "    \n",
    "    #JJ\tadjective\t'big'\n",
    "    #JJR\tadjective, comparative\t'bigger'\n",
    "    #JJS\tadjective, superlative\t'biggest'\n",
    "    # NN\tnoun, singular 'desk'\n",
    "    # NNS\tnoun plural\t'desks'\n",
    "    # NNP\tproper noun, singular\t'Harrison'\n",
    "    # NNPS\tproper noun, plural\t'Americans'\n",
    "    # VB\tverb, base form\ttake\n",
    "    # VBD\tverb, past tense\ttook\n",
    "    # VBG\tverb, gerund/present participle\ttaking\n",
    "    # VBN\tverb, past participle\ttaken\n",
    "    # VBP\tverb, sing. present, non-3d\ttake\n",
    "    # VBZ\tverb, 3rd person sing. present\ttakes\n",
    "    # RB\tadverb\tvery, silently,\n",
    "    # RBR\tadverb, comparative\tbetter\n",
    "    # RBS\tadverb, superlative\tbest\n",
    "    \n",
    "    \n",
    "    # count number of nouns, verbs, adjectives, adverbs, determiners\n",
    "    for sentence in sentences_list:\n",
    "        text = nltk.word_tokenize(sentence)\n",
    "        list_of_tags = nltk.pos_tag(text)\n",
    "        NN=0\n",
    "        VB=0\n",
    "        JJ=0\n",
    "        RB=0\n",
    "        DT=0\n",
    "    \n",
    "    for tag_tuple in list_of_tags:\n",
    "        tag = tag_tuple[1]\n",
    "        if(tag in ['NN', 'NNS', 'NNP', 'NNPS']):\n",
    "            NN +=1\n",
    "        elif(tag in ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']):\n",
    "            VB +=1\n",
    "        elif(tag in ['JJ', 'JJR', 'JJS']):\n",
    "            JJ +=1\n",
    "        elif(tag in ['RB', 'RBR', 'RBS']):\n",
    "            RB +=1\n",
    "        elif(tag == 'DT'):\n",
    "            DT +=1\n",
    "    \n",
    "    # To get Alexa-Rank of URL\n",
    "    alex_url = \"https://www.alexa.com/siteinfo/\" + url\n",
    "    alex_respone = r.get(alex_url) # get information from page\n",
    "    alex_soup = BeautifulSoup(alex_respone.content,'html.parser')  \n",
    "    for match in alex_soup.find_all('span'): #remove all span tag\n",
    "        match.unwrap()\n",
    "    global_rank = alex_soup.select('p.big.data') # select any p tag with big and data class\n",
    "    global_rank = str(global_rank[0])\n",
    "    res = re.findall(r\"([0-9,]{1,12})\", global_rank) # find rank   \n",
    "    alexa_rank=res[0]\n",
    "\n",
    "    # To get domain name of a URL\n",
    "    get_domain = urlparse(url).netloc\n",
    "    document_url_y = '.'.join(get_domain.split('.')[-1:])\n",
    "\n",
    "    if(save_flag == True):\n",
    "        # open the file in the write mode\n",
    "        with open('url_features.csv', 'a',newline='') as csvfile: \n",
    "            # csv header\n",
    "#            header = ['token_count','commas_count','exclamations_count','dots_count','questions_count',\n",
    "#                       'polarity','positive_sentences_count','negative_sentences_count',\n",
    "#                       'subjective_sentences_count','objective_sentences_count',\n",
    "#                       'spelling_errors_count','text_complexity','smog',\n",
    "#                       'noun_count','verb_count','adj_count','deter_count']\n",
    "            data = [\n",
    "                      url, tokens, commas, exclamations, dots, questions,\n",
    "                      polarity, positive_sentences, negative_sentences,\n",
    "                      subjective_sentences, objective_sentences,\n",
    "                      spelling_errors, text_complexity, smog,\n",
    "                      NN, VB, JJ, RB, DT, alexa_rank, document_url_y\n",
    "                    ]\n",
    "           \n",
    "            # create the csv writer\n",
    "            writer = csv.writer(csvfile)\n",
    "            # write header & a row to the csv file\n",
    "            # writer.writerow(header)\n",
    "            writer.writerow(data)\n",
    "    \n",
    "    feature_list=[\n",
    "                  tokens, commas, exclamations, dots, questions,\n",
    "                  polarity, positive_sentences, negative_sentences,\n",
    "                  subjective_sentences, objective_sentences,\n",
    "                  spelling_errors, text_complexity, smog,\n",
    "                  NN, VB, JJ, RB, DT, alexa_rank, document_url_y\n",
    "                 ]\n",
    "    \n",
    "    return feature_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a15b168e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b724ca28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main() function\n",
    "if __name__==\"__main__\":\n",
    "    url = input('Enter a url : ')\n",
    "    feature_list = feature_extract(url,True)\n",
    "    print(feature_list)\n",
    "    print(\"Feature list for the given URL is added to the url_features.csv file\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "114239c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"../data/web_trust.csv\",index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ce9aa625",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['document_id', 'URL', 'Credibility_rating', 'ad_count', 'ad_max_size',\n",
       "       'css_definitions', 'page_rank', 'bitly_clicks', 'bitly_referrers',\n",
       "       'tweets', 'delicious_bookmarks', 'fb_clicks', 'fb_comments', 'fb_likes',\n",
       "       'fb_shares', 'fb_total', 'alexa_linksin', 'alexa_rank', 'commas',\n",
       "       'dots', 'exclamations', 'questions', 'spelling_errors',\n",
       "       'text_complexity', 'smog', 'category', 'JJ', 'NN', 'DT', 'VB', 'RB',\n",
       "       'num_ne', 'sum_ne', 'document_url_y', 'X1', 'X2', 'X3', 'X4', 'X5',\n",
       "       'X9', 'Total', 'Leik', 'Eijk', 'Tastle', 'Leik 3 4 6', 'correction',\n",
       "       'resp_HNC', 'Controversial', 'troia_label'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28caba75",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
